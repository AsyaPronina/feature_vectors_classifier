# -*- coding: cp1251 -*-
import json

import torch
import torch.nn as nn
import torch.utils.data as data
from torch.autograd import Variable

import numpy

# Defining input size, hidden layer 1 size, hidden layer 2 size, output size respectively
n_in, n_h1, n_h2, n_out = 512, 200, 200, 6

model = nn.Sequential(nn.Linear(n_in, n_h1),
                      nn.ReLU(),
                      nn.Linear(n_h1, n_h2),
                      nn.ReLU(),
                      nn.Linear(n_h2, n_out),
                      nn.Softmax()) #fails with dim=1 - Why?

model.load_state_dict(torch.load('classifier.pt'))

# .. to load your previously training model:
darya = "-1.26519 0.643306 0.534523 -1.59837 0.531544 0.309462 0.613721 -1.35505 -1.25726 -1.17092 -0.893087 -0.00689673 -1.00818 -1.02998 1.83844 0.291772 0.362816 -0.367188 -0.129506 -1.37141 -1.62646 -0.315288 0.322262 0.545693 1.22843 0.872805 0.242985 -0.587558 0.632496 -1.16665 0.900369 -0.0949637 -0.34883 -0.933576 -1.27858 -0.463711 0.438001 -0.0823943 0.366729 -1.30683 -0.274861 0.43538 -0.759346 -0.276328 -1.00451 0.18034 2.40666 -0.111296 -0.459775 -0.450564 0.0379752 0.12385 -1.36523 1.2487 0.509896 0.915091 0.00461908 1.17501 -0.219795 -1.39124 0.24895 -0.902536 -1.37729 -0.0916655 -1.05657 0.340585 0.406676 0.269866 -1.0347 0.267827 0.224166 0.800905 -0.234378 -0.546099 0.0543157 -0.317686 -0.588683 -1.31637 0.646009 -0.279584 -0.0891728 -0.221722 -1.24258 0.313198 0.180607 -0.838682 1.91767 -1.62929 1.96466 0.300585 -0.629371 -0.569906 -0.0558117 -0.106938 -0.442249 0.100432 0.31904 -0.391584 -0.67535 -0.359388 -0.303458 -0.640691 -0.245829 -0.567219 0.467299 0.108503 -2.01974 -0.411926 -0.639627 -1.3015 1.49465 1.09565 0.1705 -0.506379 1.84643 0.313173 -0.572045 0.359517 0.181726 1.84572 0.334134 -0.279212 1.00281 -2.35137 0.0547987 -0.894184 0.305511 0.661667 -0.653649 -0.874302 -0.54545 -1.6161 -0.365999 1.74289 -1.18049 -0.304611 0.81946 -0.203173 -2.77953 0.184718 1.69811 -0.497222 1.07587 1.28507 -0.417891 -0.111153 0.160116 -0.0467821 0.737731 -0.787998 0.777586 0.833268 0.545396 0.691727 0.745496 -0.0316401 0.500534 -0.603492 -0.616032 -0.0538302 -0.577902 -0.937532 0.0515183 -0.855916 2.53727 -1.62349 2.1027 0.223727 -1.17017 0.243631 1.70726 -1.02323 -0.555918 0.278158 0.89423 0.976531 1.61431 1.53436 -0.132351 -0.442698 0.220977 0.591493 -1.21306 0.920722 0.0936093 2.21201 0.0684691 0.743398 -0.494408 0.512772 0.910102 0.24897 -0.338899 0.606412 0.449785 0.960577 -0.0543573 0.0274507 2.46015 -0.503068 -1.23812 -1.27723 -0.591513 1.1232 1.42578 -0.375301 0.399878 0.260701 -1.04118 3.25171 0.233585 -1.6083 -0.753954 0.863809 -0.960867 1.61814 -0.708456 -0.565851 0.940637 0.583663 0.0356221 1.25004 -0.221975 0.0422044 -0.41112 -0.621479 0.777595 0.0428277 0.00249576 1.62743 -0.0526339 0.883819 1.07041 1.19494 1.90916 -0.92892 0.786172 0.101361 1.60417 0.768824 -0.625778 -0.398195 -1.50747 -1.79033 -0.0751988 -0.387897 -0.217105 -1.70539 0.421261 1.20803 0.0905806 0.160235 -0.80637 -0.879303 -0.37355 -1.15109 0.345689 -0.33999 -0.290099 0.464258 -1.75033 0.98977 -0.896565 1.82597 1.38519 -0.229295 -0.346481 -1.40281 -1.185 0.29701 0.491821 -0.679272 -0.576194 0.549644 1.51333 0.499377 -0.571994 -1.28069 -0.0442203 1.10443 -0.2461 0.154318 0.349355 0.698373 -1.10426 0.777192 -0.970581 -0.236069 -0.408613 -1.06314 -1.54478 -0.585535 -0.15695 -0.358318 -0.672349 0.120356 0.147447 0.472566 1.09595 0.953325 0.117658 -0.569143 -0.762812 0.389128 0.191877 2.23673 1.21496 1.18169 -0.245542 -1.89002 0.514629 0.455703 -0.0599517 1.05546 0.73503 0.0227992 0.67623 -0.555752 -0.732552 0.526188 -0.349273 1.80538 -0.931818 -0.0270008 0.568035 0.502654 -0.435591 0.21447 0.232722 -0.986079 -1.75154 -1.42406 -0.0547469 0.512929 0.954489 -0.164228 -0.657744 0.729813 -0.654468 0.436164 -0.999422 1.95428 -1.63607 2.57939 0.710151 -0.355565 0.638921 0.441508 0.358544 0.244829 0.171373 -1.23659 2.25382 0.153346 1.51656 -2.43383 0.741007 -0.606974 1.57801 -0.666391 0.347641 -1.29536 -0.0403234 -0.8561 0.571721 0.106936 -0.415413 -0.367827 -1.53219 0.508043 -0.108168 -0.436083 -0.878391 0.68594 1.59748 0.365447 -0.120089 -0.085748 0.194753 -0.259655 0.847969 1.53001 2.00459 0.123373 -2.42289 -0.205592 -0.400784 -0.292391 -0.394626 -0.125906 -0.583638 0.313949 0.648336 0.0886338 -0.350553 -0.190004 -0.465487 1.73033 0.928203 0.745998 -1.50464 0.0342428 -0.174559 0.0507833 -0.470847 -0.737732 -0.643117 -1.02318 -0.407675 1.69442 0.527471 -0.276721 -0.128601 0.724276 -0.585072 0.490877 0.212726 -1.56998 0.191484 -0.00826531 2.71595 -1.42807 -0.796034 -0.604324 0.534176 -0.79433 -0.146486 1.18604 0.772988 -0.840836 -0.0382599 0.506939 0.17047 0.376136 1.23931 0.380166 1.08459 -1.06477 -0.626205 -0.498367 0.508123 -0.154118 0.74669 0.839857 -0.835124 -0.711504 0.66997 0.0536433 0.10303 -0.233518 -1.83699 0.292191 2.35408 -0.869462 -1.71896 0.685527 0.821239 -0.305856 0.711024 -0.569611 1.14318 -1.24446 0.34941 0.274395 0.564083 0.679745 0.734686 1.01098 0.663236 -0.507118 0.291403 0.0974975 -1.04167 1.48498 -0.477082 0.704678 2.15716 -1.063 1.60773 -0.417467 -0.499221 -1.8453 -0.714284 -0.628576 -0.493879 0.618026 1.19269 -0.581025 -0.788354 -0.387715 0.144066 1.16155 0.402561 -0.17127 0.750286 -0.675748 -1.70768 0.807116 -1.04059 0.322836 -1.2043 0.309382 0.627442 -1.20254 1.31693 1.57895 -0.708169 -0.258959 -1.69736 -1.43652 -0.563528 -0.66281"
darya =  Variable(torch.Tensor([ float(n) for n in darya.split(' ') ]))

pred = model(darya)
print("pred: ", pred)
